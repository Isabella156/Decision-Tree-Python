{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Outlook\n",
      " Outlook = Sunny ==> Test Humidity\n",
      "     Humidity = High ==> RESULT = No\n",
      "     Humidity = Normal ==> RESULT = Yes\n",
      " Outlook = Overcast ==> RESULT = Yes\n",
      " Outlook = Rain ==> Test Wind\n",
      "     Wind = Weak ==> RESULT = Yes\n",
      "     Wind = Strong ==> RESULT = No\n"
     ]
    }
   ],
   "source": [
    "run COMP2611_DT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load COMP2611_DT.py\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    \"\"\"\n",
    "\n",
    "    d.examples   A list of examples. Each one is a list of attribute values.\n",
    "    d.attr_names List of mnemonic names for corresponding attrs.\n",
    "    d.target     The attribute that a learning algorithm will try to predict.\n",
    "                 By default the final attribute.\n",
    "    d.name       Name of the data set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, examples=None, attrs=None, attr_names=None, name='', target=-1):\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        # initialize .examples from string or list or data directory\n",
    "\n",
    "        if examples is None:\n",
    "            # opening the CSV file\n",
    "            with open((name + '.csv'), 'r') as file:\n",
    "                # reading the CSV file\n",
    "                csvFile = csv.reader(file)\n",
    "                attr_names = next(csvFile)\n",
    "                self.examples = list(csv.reader(file))\n",
    "        else:\n",
    "            self.examples = examples\n",
    "\n",
    "        # attrs are the indices of examples, unless otherwise stated.\n",
    "        if self.examples is not None and attrs is None:\n",
    "            attrs = list(range(len(self.examples[0])))\n",
    "\n",
    "        self.attrs = attrs\n",
    "\n",
    "        # initialize .attr_names from string, list, or by default\n",
    "        if isinstance(attr_names, str):\n",
    "            self.attr_names = attr_names.split()\n",
    "        else:\n",
    "            self.attr_names = attr_names or attrs\n",
    "\n",
    "        self.target = self.attr_num(target)\n",
    "\n",
    "        self.inputs = [a for a in self.attrs if a != self.target]\n",
    "        # find possible range of values for attributes\n",
    "        self.values = list(map(unique, zip(*self.examples)))\n",
    "\n",
    "    def attr_num(self, attr):\n",
    "        \"\"\"Returns the number used for attr, which can be a name, or -n .. n-1.\"\"\"\n",
    "        if isinstance(attr, str):\n",
    "            return self.attr_names.index(attr)\n",
    "        elif attr < 0:\n",
    "            return len(self.attrs) + attr\n",
    "        else:\n",
    "            return attr\n",
    "\n",
    "    def sanitize(self, example):\n",
    "        \"\"\"Return a copy of example, with non-input attributes replaced by None.\"\"\"\n",
    "        return [attr_i if i in self.inputs else None for i, attr_i in enumerate(example)]\n",
    "\n",
    "    def classes_to_numbers(self, classes=None):\n",
    "        \"\"\"Converts class names to numbers.\"\"\"\n",
    "        if not classes:\n",
    "            # if classes were not given, extract them from values\n",
    "            classes = sorted(self.values[self.target])\n",
    "        for item in self.examples:\n",
    "            item[self.target] = classes.index(item[self.target])\n",
    "\n",
    "    def remove_examples(self, value=''):\n",
    "        \"\"\"Remove examples that contain given value.\"\"\"\n",
    "        self.examples = [x for x in self.examples if value not in x]\n",
    "        self.update_values()\n",
    "\n",
    "    def split_values_by_classes(self):\n",
    "        \"\"\"Split values into buckets according to their class.\"\"\"\n",
    "        buckets = defaultdict(lambda: [])\n",
    "        target_names = self.values[self.target]\n",
    "\n",
    "        for v in self.examples:\n",
    "            item = [a for a in v if a not in target_names]  # remove target from item\n",
    "            buckets[v[self.target]].append(item)  # add item to bucket of its class\n",
    "\n",
    "        return buckets\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<DataSet({}): {:d} examples, {:d} attributes>'.format(self.name, len(self.examples), len(self.attrs))\n",
    "\n",
    "\n",
    "def err_ratio(predict, dataset, examples=None):\n",
    "    \"\"\"\n",
    "    Return the proportion of the examples that are NOT correctly predicted.\n",
    "    verbose - 0: No output; 1: Output wrong; 2 (or greater): Output correct\n",
    "    \"\"\"\n",
    "    examples = examples or dataset.examples\n",
    "    if len(examples) == 0:\n",
    "        return 0.0\n",
    "    right = 0\n",
    "    for example in examples:\n",
    "        desired = example[dataset.target]\n",
    "        output = predict(dataset.sanitize(example))\n",
    "        if output == desired:\n",
    "            right += 1\n",
    "\n",
    "    return 1 - (right / len(examples))\n",
    "\n",
    "\n",
    "def grade_learner(predict, tests):\n",
    "    \"\"\"\n",
    "    Grades the given learner based on how many tests it passes.\n",
    "    tests is a list with each element in the form: (values, output).\n",
    "    \"\"\"\n",
    "    return mean(int(predict(X) == y) for X, y in tests)\n",
    "\n",
    "\n",
    "def train_test_split(dataset, start=None, end=None, test_split=None):\n",
    "    \"\"\"\n",
    "        If you are giving 'start' and 'end' as parameters,\n",
    "        then it will return the testing set from index 'start' to 'end'\n",
    "        and the rest for training.\n",
    "        If you give 'test_split' as a parameter then it will return\n",
    "        test_split * 100% as the testing set and the rest as\n",
    "        training set.\n",
    "        \"\"\"\n",
    "    examples = dataset.examples\n",
    "    if test_split is None:\n",
    "        train = examples[:start] + examples[end:]\n",
    "        val = examples[start:end]\n",
    "    else:\n",
    "        total_size = len(examples)\n",
    "        val_size = int(total_size * test_split)\n",
    "        train_size = total_size - val_size\n",
    "        train = examples[:train_size]\n",
    "        val = examples[train_size:total_size]\n",
    "\n",
    "    train_set = DataSet(examples=train, attr_names=dataset.attr_names, attrs=dataset.attrs, target=dataset.target)\n",
    "    val_set = DataSet(examples=val, attr_names=dataset.attr_names, attrs=dataset.attrs, target=dataset.target)\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "def PluralityLearner(dataset):\n",
    "    \"\"\"\n",
    "    A very dumb algorithm: always pick the result that was most popular\n",
    "    in the training data. Makes a baseline for comparison.\n",
    "    \"\"\"\n",
    "    most_popular = mode([e[dataset.target] for e in dataset.examples])\n",
    "\n",
    "    def predict(example):\n",
    "        \"\"\"Always return same result: the most popular from the training set.\"\"\"\n",
    "        return most_popular\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "class DecisionFork:\n",
    "    \"\"\"\n",
    "    A fork of a decision tree holds an attribute to test, and a dict\n",
    "    of branches, one for each of the attribute's values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attr, attr_name=None, default_child=None, branches=None, parent=None):\n",
    "        \"\"\"Initialize by saying what attribute this node tests.\"\"\"\n",
    "        self.attr = attr\n",
    "        self.attr_name = attr_name or attr\n",
    "        self.default_child = default_child\n",
    "        self.branches = branches or {}\n",
    "        self.pos = 0\n",
    "        self.neg = 0\n",
    "        self.parent_node = parent\n",
    "\n",
    "    def __call__(self, example, target=None):\n",
    "        \"\"\"Given an example, classify it using the attribute and the branches.\"\"\"\n",
    "        attr_val = example[self.attr]\n",
    "\n",
    "        if (target):\n",
    "            if (example[target] == \"Yes\"):\n",
    "                self.pos = self.pos + 1\n",
    "            else:\n",
    "                self.neg = self.neg + 1\n",
    "\n",
    "        if attr_val in self.branches:\n",
    "            return self.branches[attr_val](example, target)\n",
    "        else:\n",
    "            print(\"attr not found \", attr_val)\n",
    "            # return default class when attribute is unknown\n",
    "            return self.default_child(example)\n",
    "\n",
    "    def clear_count(self):\n",
    "        self.pos = 0\n",
    "        self.neg = 0\n",
    "        return (0)\n",
    "\n",
    "    def add(self, val, subtree):\n",
    "        \"\"\"Add a branch. If self.attr = val, go to the given subtree.\"\"\"\n",
    "        self.branches[val] = subtree\n",
    "\n",
    "    def display(self, indent=0):\n",
    "        name = self.attr_name\n",
    "        print('Test', name)\n",
    "        for (val, subtree) in self.branches.items():\n",
    "            print(' ' * 4 * indent, name, '=', val, '==>', end=' ')\n",
    "            subtree.display(indent + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'DecisionFork({0!r}, {1!r}, {2!r})'.format(self.attr, self.attr_name, self.branches)\n",
    "\n",
    "\n",
    "class DecisionLeaf:\n",
    "    \"\"\"A leaf of a decision tree holds just a result.\"\"\"\n",
    "\n",
    "    def __init__(self, result, parent=None):\n",
    "        self.pos = 0\n",
    "        self.neg = 0\n",
    "        self.result = result\n",
    "        self.parent_node = parent\n",
    "\n",
    "    #    def __call__(self, example):\n",
    "    #        return self.result\n",
    "\n",
    "    def __call__(self, example, target=None):\n",
    "\n",
    "        if (target):\n",
    "            if (example[target] == \"Yes\"):\n",
    "                self.pos = self.pos + 1\n",
    "            else:\n",
    "                self.neg = self.neg + 1\n",
    "\n",
    "        return self.result\n",
    "\n",
    "    def clear_count(self):\n",
    "        self.pos = 0\n",
    "        self.neg = 0\n",
    "        return (0)\n",
    "\n",
    "    def display(self, indent=0):\n",
    "        print('RESULT =', self.result)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.result)\n",
    "\n",
    "\n",
    "def DecisionTreeLearner(dataset):\n",
    "    target, values = dataset.target, dataset.values\n",
    "\n",
    "    def decision_tree_learning(examples, attrs, parent_examples=()):\n",
    "        if len(examples) == 0:\n",
    "            return plurality_value(parent_examples)\n",
    "        if all_same_class(examples):\n",
    "            return DecisionLeaf(examples[0][target])\n",
    "        if len(attrs) == 0:\n",
    "            return plurality_value(examples)\n",
    "        A = choose_attribute(attrs, examples)\n",
    "        tree = DecisionFork(A, dataset.attr_names[A], plurality_value(examples))\n",
    "        for (v_k, exs) in split_by(A, examples):\n",
    "            subtree = decision_tree_learning(exs, remove_all(A, attrs), examples)\n",
    "            tree.add(v_k, subtree)\n",
    "        return tree\n",
    "\n",
    "    def plurality_value(examples):\n",
    "        \"\"\"\n",
    "        Return the most popular target value for this set of examples.\n",
    "        (If target is binary, this is the majority; otherwise plurality).\n",
    "        \"\"\"\n",
    "        np.random.seed(1915)\n",
    "        popular = argmax_random_tie(values[target], key=lambda v: count(target, v, examples))\n",
    "\n",
    "        return DecisionLeaf(popular)\n",
    "\n",
    "    def count(attr, val, examples):\n",
    "        \"\"\"Count the number of examples that have example[attr] = val.\"\"\"\n",
    "        return sum(e[attr] == val for e in examples)\n",
    "\n",
    "    def all_same_class(examples):\n",
    "        \"\"\"Are all these examples in the same target class?\"\"\"\n",
    "        class0 = examples[0][target]\n",
    "        return all(e[target] == class0 for e in examples)\n",
    "\n",
    "    def choose_attribute(attrs, examples):\n",
    "        \"\"\"Choose the attribute with the highest information gain.\"\"\"\n",
    "        np.random.seed(1915)\n",
    "        return argmax_random_tie(attrs, key=lambda a: information_gain(a, examples))\n",
    "\n",
    "    def information_gain(attr, examples):\n",
    "        \"\"\"Return the expected reduction in entropy from splitting by attr.\"\"\"\n",
    "\n",
    "        def I(examples):\n",
    "            return information_content([count(target, v, examples) for v in values[target]])\n",
    "\n",
    "        n = len(examples)\n",
    "        remainder = sum((len(examples_i) / n) * I(examples_i) for (v, examples_i) in split_by(attr, examples))\n",
    "        return I(examples) - remainder\n",
    "\n",
    "    def split_by(attr, examples):\n",
    "        \"\"\"Return a list of (val, examples) pairs for each val of attr.\"\"\"\n",
    "        return [(v, [e for e in examples if e[attr] == v]) for v in values[attr]]\n",
    "\n",
    "    return decision_tree_learning(dataset.examples, dataset.inputs)\n",
    "\n",
    "\n",
    "def information_content(values):\n",
    "    \"\"\"Number of bits to represent the probability distribution in values.\"\"\"\n",
    "    probabilities = normalize(remove_all(0, values))\n",
    "    return sum(-p * np.log2(p) for p in probabilities)\n",
    "\n",
    "\n",
    "# Task 4c\n",
    "def deviation(value, parent_pos, parent_neg):\n",
    "    ##actual counts of exmaples at this node are given by value.pos and value.neg\n",
    "    ##actula counts at its parents are parent.pos and parent.neg\n",
    "    ##function must return the sqaured difference between the actual and expected counts.\n",
    "    deviation = 0\n",
    "\n",
    "    if (value.pos == 0 and value.neg == 0):\n",
    "        return 0\n",
    "\n",
    "    # Insert code here\n",
    "\n",
    "    return (deviation)\n",
    "\n",
    "\n",
    "def replaceFork(parent, leaf):\n",
    "    if (parent.parent_node == None):  # tries to handle removal of top node (has no parents)\n",
    "        for key, value in list(parent.branches.items()):  # make all branches same leaf\n",
    "            parent.branches[key] = leaf\n",
    "    else:\n",
    "        for key, value in list(parent.parent_node.branches.items()):\n",
    "            if value == parent:\n",
    "                parent.parent_node.branches[key] = leaf\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def order(tree):\n",
    "    def decisiontree_iterator(parent):\n",
    "        ''' This function accepts a node as an argument\n",
    "        and iterate over all values of its children to clear examples counts\n",
    "        '''\n",
    "\n",
    "        parent.branches = dict(sorted(parent.branches.items(), key=lambda kv: kv[0]))\n",
    "\n",
    "        # Iterate over all key-value pairs of DecisionFrok argument\n",
    "        for key, value in list(parent.branches.items()):\n",
    "            if isinstance(value, DecisionFork):\n",
    "                yield from decisiontree_iterator(value)\n",
    "\n",
    "    all_nodes = list(decisiontree_iterator(tree))\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def clear_counts(tree):\n",
    "    def decisiontree_iterator(parent):\n",
    "        ''' This function accepts a node as an argument\n",
    "        and iterate over all values of its children to clear examples counts\n",
    "        '''\n",
    "        # Iterate over all key-value pairs of DecisionFrok argument\n",
    "        for key, value in list(parent.branches.items()):\n",
    "            value.clear_count()\n",
    "            if isinstance(value, DecisionFork):\n",
    "                yield from decisiontree_iterator(value)\n",
    "\n",
    "    all_nodes = list(decisiontree_iterator(tree))\n",
    "\n",
    "\n",
    "# Task 4d\n",
    "def evaluate(predict, dataset, examples=None):\n",
    "    \"\"\"\n",
    "    Return the proportion of the examples that are NOT correctly predicted.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def decisiontree_iterator(parent):\n",
    "        ''' This function accepts a node as an argument\n",
    "        and iterate over all values of its children\n",
    "        '''\n",
    "        p_value = 0\n",
    "        DELTA = -1.0\n",
    "        further_fork = False  # used to flag if fork only has leaf nodes\n",
    "\n",
    "        # Iterate over all key-value pairs of DecisionFrok argument\n",
    "        for key, value in list(parent.branches.items()):\n",
    "            value.parent_node = parent  # makes it easier to prune back\n",
    "            if isinstance(value, DecisionFork):\n",
    "                further_fork = True\n",
    "                # yield from decisiontree_iterator(value)\n",
    "                p_value, DELTA = decisiontree_iterator(value)\n",
    "\n",
    "        # Is a fork with only leaf nodes therefore could be pruned\n",
    "        if further_fork == False:\n",
    "\n",
    "            if (parent.pos > 0 and parent.neg > 0):\n",
    "\n",
    "                DELTA = sum(deviation(value, parent.pos, parent.neg) for key, value in parent.branches.items())\n",
    "\n",
    "                # Insert code here\n",
    "                # calculate p_value using the stats.chi2.cdf function.\n",
    "                # The degree of freedom (num of variable) is the number of branches at the parent.\n",
    "\n",
    "                print(\"chisquare-score is:\", DELTA, \" and p value is:\", p_value)\n",
    "\n",
    "                if p_value <= 0.05:\n",
    "                    print(\"Null Hypothesis is rejected.\")\n",
    "                else:\n",
    "                    print(\"Failed to reject the Null hypothesis.\")\n",
    "                    print(\"Pruning\")\n",
    "                    # can prune parent\n",
    "                    if (parent.pos > parent.neg):\n",
    "                        replaceFork(parent, DecisionLeaf(\"Yes\"))\n",
    "                    else:\n",
    "                        replaceFork(parent, DecisionLeaf(\"No\"))\n",
    "\n",
    "        return (p_value, DELTA)\n",
    "\n",
    "    examples = examples or dataset.examples\n",
    "    if len(examples) == 0:\n",
    "        return 0.0\n",
    "    right = 0\n",
    "\n",
    "    target = dataset.target\n",
    "\n",
    "    # predict outcome for each exmaple\n",
    "    for example in examples:\n",
    "        desired = example[dataset.target]\n",
    "        output = predict(example, target)\n",
    "        if output == desired:\n",
    "            right += 1\n",
    "\n",
    "    p_value, DELTA = decisiontree_iterator(predict)\n",
    "\n",
    "    return (p_value, DELTA, 1 - (right / len(examples)))\n",
    "\n",
    "\n",
    "def RestaurantDataSet(examples=None):\n",
    "    \"\"\"\n",
    "   [Figure 19.3]\n",
    "   Build a DataSet of Restaurant waiting examples.\n",
    "   \"\"\"\n",
    "    return DataSet(name='restaurant1', target='Wait', examples=examples,\n",
    "                   attr_names='Alternate Bar Fri/Sat Hungry Patrons Raining Reservation WaitEstimate Wait')\n",
    "\n",
    "\n",
    "restaurant = RestaurantDataSet()\n",
    "\n",
    "\n",
    "def T(attr_name, branches):\n",
    "    branches = {value: (child if isinstance(child, DecisionFork) else DecisionLeaf(child))\n",
    "                for value, child in branches.items()}\n",
    "    return DecisionFork(attr=restaurant.attr_num(attr_name), attr_name=attr_name, default_child=print,\n",
    "                        branches=branches)\n",
    "\n",
    "\n",
    "waiting_decision_tree = T('Patrons',\n",
    "                          {'None': 'No', 'Some': 'Yes',\n",
    "                           'Full': T('WaitEstimate',\n",
    "                                     {'>60': 'No', '0-10': 'Yes',\n",
    "                                      '30-60': T('Alternate',\n",
    "                                                 {'No': T('Reservation',\n",
    "                                                          {'Yes': 'Yes',\n",
    "                                                           'No': T('Bar', {'No': 'No',\n",
    "                                                                           'Yes': 'Yes'})}),\n",
    "                                                  'Yes': T('Fri/Sat', {'No': 'No', 'Yes': 'Yes'})}),\n",
    "                                      '10-30': T('Hungry',\n",
    "                                                 {'No': 'Yes',\n",
    "                                                  'Yes': T('Alternate',\n",
    "                                                           {'No': 'Yes',\n",
    "                                                            'Yes': T('Raining',\n",
    "                                                                     {'No': 'No',\n",
    "                                                                      'Yes': 'Yes'})})})})})\n",
    "\n",
    "\n",
    "def SyntheticRestaurantPruneTest(n=100):\n",
    "    \"\"\"Generate a DataSet with n examples.\"\"\"\n",
    "    np.random.seed(4000)\n",
    "\n",
    "    def gen():\n",
    "        example = list(map(np.random.choice, restaurant.values))\n",
    "        example[restaurant.target] = waiting_decision_tree(example)\n",
    "\n",
    "        rand = np.random.random_sample()\n",
    "        if rand >= 0.2:\n",
    "            rand = np.random.random_sample()\n",
    "            if rand >= 0.5:\n",
    "                if example[5] == \"Yes\":\n",
    "                    example[5] = \"No\"\n",
    "                else:\n",
    "                    if example[1] == \"Yes\":\n",
    "                        example[1] = \"No\"\n",
    "\n",
    "        return example\n",
    "\n",
    "    return RestaurantDataSet([gen() for _ in range(n)])\n",
    "\n",
    "\n",
    "def SyntheticRestaurantTest(n=100):\n",
    "    \"\"\"Generate a DataSet with n examples.\"\"\"\n",
    "    np.random.seed(4000)\n",
    "\n",
    "    def gen():\n",
    "        example = list(map(np.random.choice, restaurant.values))\n",
    "\n",
    "        rand = np.random.random_sample()\n",
    "        if rand >= 1.1:\n",
    "            if example[restaurant.target] == \"Yes\":\n",
    "                example[restaurant.target] = \"No\"\n",
    "\n",
    "            else:\n",
    "                example[restaurant.target] == \"Yes\"\n",
    "\n",
    "        return example\n",
    "\n",
    "    return RestaurantDataSet([gen() for _ in range(n)])\n",
    "\n",
    "\n",
    "def SyntheticRestaurant(n=100):\n",
    "    \"\"\"Generate a DataSet with n examples.\"\"\"\n",
    "\n",
    "    np.random.seed(2000)\n",
    "\n",
    "    def gen():\n",
    "        example = list(map(np.random.choice, restaurant.values))\n",
    "\n",
    "        example[restaurant.target] = waiting_decision_tree(example)\n",
    "        return example\n",
    "\n",
    "    return RestaurantDataSet([gen() for _ in range(n)])\n",
    "\n",
    "\n",
    "# TASK 1\n",
    "\n",
    "def learn_tennis_tree(filename):\n",
    "    \"\"\"\n",
    "    function should create a decision tree from the file named filename\n",
    "    returns the data set used to create the tree and the learnt decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    # insert code here\n",
    "    dataSet = DataSet(name=filename, target='Play')\n",
    "    tree = DecisionTreeLearner(dataSet)\n",
    "    tree.display()\n",
    "\n",
    "    return (dataSet, tree)\n",
    "\n",
    "\n",
    "# TASK 2\n",
    "\n",
    "def test_tennis_tree(filename):\n",
    "    \"\"\"\n",
    "    function should split the data provided by filename into a traing and test set.\n",
    "    earn a decision tree from the training set\n",
    "    test the tree on the test and evaluate its performance\n",
    "    returns a the training and test sets, the decion tree and the error rate achieved.\n",
    "    \"\"\"\n",
    "\n",
    "    # insert code here\n",
    "    dataSet, tree = learn_tennis_tree(filename)\n",
    "    trainSet, testSet = train_test_split(dataSet, test_split=0.2)\n",
    "    error = err_ratio(tree, testSet)\n",
    "\n",
    "    return trainSet, testSet, tree, error\n",
    "\n",
    "\n",
    "# TASK 3a\n",
    "def genSyntheticTrainSet():\n",
    "    ##function generates a synthetic data set using the SytheticRestatuant method\n",
    "    ##returns the dataset created\n",
    "    data = None\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (data)\n",
    "\n",
    "\n",
    "# TASK 3b\n",
    "def genSyntheticTestSet():\n",
    "    ##function generates a synthetic data set using the SytheticRestatuantTest method\n",
    "    ##returns the dataset created\n",
    "    data = None\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (data)\n",
    "\n",
    "\n",
    "# TASK 3c\n",
    "def train_restaurant_tree(trainSet, testSet, N=200):\n",
    "    ## function should learn decision trees using different quantities of the training set (trainSet) from 1 to N\n",
    "    ## where N should be the total size of the training set (trainSet)\n",
    "    ## and test each tree on the whole test set (testSet) provided\n",
    "    ## the function should return the final tree obtained using all 200 samples,\n",
    "    ## and the minimum size of the training set (samples) required to achieve the same error rate as achieved using all 200 training samples.\n",
    "\n",
    "    tree = None\n",
    "    samples_required = 0\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (tree, samples_required)\n",
    "\n",
    "\n",
    "# TASK 3d\n",
    "def train_tree(trainSet, testSet):\n",
    "    ## function should learn a decision tree the training set (trainSet)\n",
    "    ## and test the tree on the whole test set (testSet)\n",
    "    ## the method should return the tree, and the error rate achieved.\n",
    "    tree = None\n",
    "    error = 0\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (tree, error)\n",
    "\n",
    "\n",
    "# TASK 4a\n",
    "def genPruneTestSet():\n",
    "    ##function generates a synthetic data set using the SytheticRestaruantPruneTest method\n",
    "    ##returns the dataset created\n",
    "    data = None\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (data)\n",
    "\n",
    "\n",
    "# TASK 4b\n",
    "def prune_tree(tree, testSet):\n",
    "    ##function should prune the decison tree (tree) using the evaluate method as many times as required when evaluated using testSet.\n",
    "    ##the function must return the testSet used, the p_value, K and error rates of the final tree (tree) returned from the evalaute function.\n",
    "\n",
    "    p_value = 0\n",
    "    error_rate = 0\n",
    "    delta = 1.0\n",
    "\n",
    "    # insert code here\n",
    "\n",
    "    return (testSet, p_value, delta, tree, error_rate)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = \"./tennis\"\n",
    "    tennis_dataSet, tree = learn_tennis_tree(filename)  # task 1\n",
    "\n",
    "    # error_rate = test_tennis_tree(filename)  # task 2\n",
    "    # print(\"Error_rate \", error_rate)\n",
    "    \"\"\"\n",
    "    train_set = genSyntheticTrainSet()  # task 3a\n",
    "    test_set = genSyntheticTestSet()  # task 3b\n",
    "    restaurant_tree, errors = train_restaurant_tree(train_set, test_set)  # task 3c\n",
    "\n",
    "    tree, error_rate = train_tree(train_set, test_set)  # task 2d\n",
    "\n",
    "    testData = genPruneTestSet()  # task 4a\n",
    "    testData, p_value, delta, pruned_tree, error = prune_tree(tree, testData)  # task 4b,c and d\n",
    "    print(\"pruned error rate \", error)\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
